1. find the best neural net (based on observed error rate)
Complexity to ignore: all the nuances of how we choose when to determine this. just pick something (after x minutes || Y iterations; i think that's what i have in there now- don't worry, it's good enough)
2. find the best random forest- 
    tweak all the parameters available here
      once we have tweaked them all and found the best combo, bump up the number of trees significantly, since we know that will almost always increase accuracy, no matter the other parameters. 
    each child process should just be one combination of parameters- exactly as it is for the JS stuff
      create a readFile script that reads in the data in the file, and if necessary, formats the data in the way python expects
      send messages back and forth from teh child process just like we do for the NNs

3. assume that just averaging them together is the best way
4. focus this ONLY on kaggle competitions for now- don't worry about saving the models, or accepting user input for predictions, etc. ONLY focus on taking in two data sets (training, and testing), and running on those. 


what's the value here-
  we let the ML engineers focus solely on feature extraction
  we handle the entire act of figuring out which NN, or which combination of parameters for a RF, is most effective
  and we handle all the data formatting, so if you just figure out what features you want to extract and put those in the .csv file, we'll handle getting all those things ready for the various ml algos

test point:
  test it on all the kaggle competitions. see if i can get my own personal kaggle score up there

future: 
  A. little script for uploading results directly to kaggle (fetching them from kaggle too?!). 
  1. more advanced ways of aggregating the different models together
      averaging
      taking the highest
      taking the lowest predicted value out of all of them
      randomly choosing one of the predictions
      taking any outlier values (something like MAX(ABS(50-PredictedValue)) to find which is the most extreme prediction (either high or low), and taking that)
      taking any value two or more algos cluster around (take the min of the differences between predictions, and then average only those two predictions)
      taking the trainingErrorRate of the different models into account (if one model is 100% accurate on the training set, and the other model is 50% accurate, weighting the prediction from the 100% model twice as much- obviously the math here would be different, but in super basic terms, that's the idea)
      maybe make sure that we don't include any models that are notably less accurate than others
      try including models from the most accurate down! what if we included only the most accurate model? ok, what if we included the two most accurate models? ok, what if we included the three most accurate models? 
        we could even take this approach in conjunction with the others. what if we took the two most accurate models, but each time took the highest predicted value; the lowest predicted value, the outlier value, etc. ok, now cycle through them all again assuming we've got the three best models, now again with the four best models, etc. 

  2. More Models
      KNN
      straight regression
      SVM
      anything else scikit learn has available, since the data formatting's free
      since we're aggregating algos together in such creative ways, there's very little danger to adding in more algos, so i say we just add in as many as we reasonably can

  2A. allow the user to determine the methodology used for calculating the error rate. right now we're using the error rate on the training set. what if we wanted to let the user pass in the kaggle testing data set, run the algo against that, upload it to kaggle, get the observed result, and use that as the error rate? certainly would be difficult, but would probably increase accuracy dramatically

  2A. feature selection- use some kind of automated tool to do feature selection for us. 

  2B. allow the user to run some code to create features as part of our initial kickoff. 
      they give us a code snippet (something that is node compatible and designed to work on a single item). we run that snippet of code on every row in the dataset as part of our first stream (we'd have a whole kickoff stream dedicated to running this code and creating these new features). i don't like this idea much. i'd rather have them create those features manually outside of ppComplete and just have them harcoded into the data file, so they can keep an eye on those features. the benefit of this is just increasing ease even more- they don't need to worry about reading or writing to files, setting up a node server, etc. 

  3. voluntary data gathering to run ML on our own library
      once trained, ask users if they'd be ok sharing some summary info about their data and the parameters of their models (totally anonymized) with us
      we could then figure out what combinations of params work best for data of a certain shape (num features, standard dev, min, max, range, num data points, type of predicted output, num of categories being predicted, etc.). 
      offer a "Quickstart" option- default to whatever we predict will be the best params for your data set, and train algos with those- don't traverse the param space trying all combos of params. 

  4. create a UI to make this even more accessible (and create pretty visualizations while training!), and collect that summarized data by default (with very obvious notifications about this and a way to turn it off)

  5. create an automated deployment script for aws so people can run this on a big box in the cloud super easily

  6. offer to run this on big boxes in the cloud for people automatically, given a fee. 
  


